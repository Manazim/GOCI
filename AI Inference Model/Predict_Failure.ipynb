!pip install pymongo pandas scikit-learn tensorflow matplotlib

import pandas as pd
import numpy as np
import requests
from pymongo import MongoClient
from pymongo.server_api import ServerApi
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import matplotlib.pyplot as plt

# --------------------------
# Function to get ambient temperature from OpenWeather API
def get_ambient_temperature(city="#######", api_key="###################################"):
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
    try:
        response = requests.get(url)
        data = response.json()
        ambient_temp = data['main']['temp']
        return ambient_temp
    except Exception as e:
        print("Error fetching ambient temperature:", e)
        return 20  # default value

# --------------------------
# 1. Connect to MongoDB and fetch sensor data
mongo_uri = "##################################################"
client = MongoClient(mongo_uri, server_api=ServerApi('1'))
db_collection = client.GOCI.sensors

cursor = db_collection.find({})
data_list = list(cursor)
df = pd.DataFrame(data_list)
if '_id' in df.columns:
    df = df.drop(columns=['_id'])

# Convert Timestamp to datetime and sort
df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%Y-%m-%d %H:%M:%S')
df = df.sort_values('Timestamp')

print("Data from MongoDB:")
print(df.head())

# 2. Use the last 500 readings
if len(df) >= 500:
    df = df.iloc[-500:]
else:
    print("Warning: less than 500 records available.")

# Define features to use
features = ['Pressure', 'Flow_rate', 'Water_quality', 'Temperature']
data = df[features].values  # shape: (500, 4)

# 3. Normalize the data to [0,1]
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# 4. Split data: first 380 readings for training, last 120 for testing
train_size = 380
train_data = scaled_data[:train_size]  # shape: (380, 4)
test_data = scaled_data[train_size:]   # shape: (120, 4)

# 5. Create training sequences using a sliding window
time_step = 20
def create_dataset(dataset, time_step):
    X, y = [], []
    for i in range(len(dataset) - time_step):
        X.append(dataset[i:i+time_step])
        y.append(dataset[i+time_step])
    return np.array(X), np.array(y)

X_train, y_train = create_dataset(train_data, time_step)
print(f"Training samples: {X_train.shape[0]}")

# 6. Build and train an LSTM model for multivariate output
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(time_step, len(features))))
model.add(LSTM(50))
model.add(Dense(len(features)))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)

# Plot training history
plt.figure(figsize=(8,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# 7. Recursive multi-step forecasting for 120 steps
num_steps = 120
input_seq = train_data[-time_step:]  # last time_step readings from training data
predictions = []
for _ in range(num_steps):
    pred = model.predict(input_seq[np.newaxis, :, :], verbose=0)
    predictions.append(pred[0])
    input_seq = np.concatenate([input_seq[1:], pred], axis=0)
predictions = np.array(predictions)  # shape: (120, 4)

# 8. Inverse transform predictions and actual test data to original scale
predictions_inv = scaler.inverse_transform(predictions)
test_data_inv = scaler.inverse_transform(test_data)

# 9. Debug: Print predicted vs. actual values for first 5 samples
print("\nPredictions vs Actual for first 5 samples:")
for i in range(min(5, predictions_inv.shape[0])):
    print(f"Sample {i+1}:")
    for j, feature in enumerate(features):
        print(f"  {feature} - Predicted: {predictions_inv[i][j]:.2f}, Actual: {test_data_inv[i][j]:.2f}")

# 10. Anomaly detection:
# Get ambient temperature and set dynamic threshold for Temperature anomaly.
ambient_temp = get_ambient_temperature()
print("\nAmbient Temperature (from API):", ambient_temp, "°C")

# Determine dynamic temperature anomaly threshold
if ambient_temp >= 30 or ambient_temp <= 10:
    dynamic_threshold_temp = 5.0
else:
    dynamic_threshold_temp = 2.0

print("Dynamic Temperature Anomaly Threshold:", dynamic_threshold_temp, "°C")


# Define fixed thresholds for other features
thresholds = {
    'Pressure': 5.0,
    'Flow_rate': 5.0,
    'Water_quality': 5.0,
    'Temperature': dynamic_threshold_temp
}

# Compare over overlapping region
n_compare = min(test_data_inv.shape[0], predictions_inv.shape[0])

# Compute anomaly flags for each feature
pressure_flags = np.abs(test_data_inv[:n_compare, 0] - predictions_inv[:n_compare, 0]) > thresholds['Pressure']
flow_flags     = np.abs(test_data_inv[:n_compare, 1] - predictions_inv[:n_compare, 1]) > thresholds['Flow_rate']
wq_flags       = np.abs(test_data_inv[:n_compare, 2] - predictions_inv[:n_compare, 2]) > thresholds['Water_quality']
temp_flags     = np.abs(test_data_inv[:n_compare, 3] - predictions_inv[:n_compare, 3]) > thresholds['Temperature']

# Combine Pressure and Flow anomalies for Leakage detection
leakage_flags = pressure_flags & flow_flags

# 11. Calculate anomaly percentages for each
leakage_percent = np.mean(leakage_flags) * 100
wq_percent = np.mean(wq_flags) * 100
temp_percent = np.mean(temp_flags) * 100

# Print detailed anomaly flags for first 5 samples (optional)
print("\nDetailed anomaly flags for first 5 samples:")
for i in range(min(5, n_compare)):
    print(f"Sample {i+1}:")
    print(f"  Pressure anomaly: {pressure_flags[i]}, Flow anomaly: {flow_flags[i]}, Combined Leakage: {leakage_flags[i]}")
    print(f"  Water Quality anomaly: {wq_flags[i]}, Temperature anomaly: {temp_flags[i]}")

# 12. Final anomaly summary
print("\nAnomaly Summary:")
print(f"Leakage (Pressure & Flow): {leakage_percent:.2f}% of readings flagged as potential leakage")
print(f"Water Quality Drop: {wq_percent:.2f}% of readings flagged")
print(f"Temperature Anomaly (with ambient adjustment): {temp_percent:.2f}% of readings flagged")

# 13. Plot comparisons for each feature
plt.figure(figsize=(14,10))
for idx, feature in enumerate(features):
    plt.subplot(2,2,idx+1)
    plt.plot(test_data_inv[:n_compare, idx], label='Actual')
    plt.plot(predictions_inv[:n_compare, idx], label='Predicted')
    plt.title(feature)
    plt.xlabel('Time Step')
    plt.ylabel(feature)
    plt.legend()
plt.tight_layout()
plt.show()

